# app/models.py
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any


class EvaluationRequest(BaseModel):
    """Request model for LLM response evaluation."""
    app_name: str = Field(..., description="Name of the application being evaluated")
    user: str = Field(..., description="User identifier")
    user_request: str = Field(..., description="Original request or question from the user")
    app_actual_response: str = Field(..., description="Actual response generated by the LLM")
    expected_response: Optional[str] = Field(None, description="Expected or reference response (optional for some modes)")
    context: Optional[str] = Field(None, description="Context information provided to the LLM (required for RAG mode)")
    mode: Optional[str] = Field("standard", description="Evaluation mode: 'quick', 'standard', 'rag', 'agent', 'complete'")


class EvaluationResponse(BaseModel):
    """Response model containing evaluation metrics."""
    app_name: str = Field(..., alias="App Name")
    user: str = Field(..., alias="User")
    user_request: str = Field(..., alias="User Request")
    actual_output: str = Field(..., alias="Actual Output")
    expected_output: Optional[str] = Field(None, alias="Expected Output")
    context: Optional[str] = Field(None, alias="Context")
    mode: str = Field(..., alias="Evaluation Mode")
    
    # Metrics - all optional since they depend on the mode
    answer_relevancy_score: Optional[float] = Field(None, alias="Answer Relevancy Score")
    faithfulness_score: Optional[float] = Field(None, alias="Faithfulness Score")
    hallucination_score: Optional[float] = Field(None, alias="Hallucination Score")
    contextual_relevancy_score: Optional[float] = Field(None, alias="Contextual Relevancy Score")
    contextual_precision_score: Optional[float] = Field(None, alias="Contextual Precision Score")
    contextual_recall_score: Optional[float] = Field(None, alias="Contextual Recall Score")
    bias_score: Optional[float] = Field(None, alias="Bias Score")
    toxicity_score: Optional[float] = Field(None, alias="Toxicity Score")
    
    # For future expansion (token usage)
    # token_usage: Optional[Dict[str, int]] = Field(None, alias="Token Usage")
    # total_tokens: Optional[int] = Field(None, alias="Total Tokens Used")
    # estimated_cost: Optional[float] = Field(None, alias="Estimated Cost (USD)")
    
    # Detailed feedback (optional)
    evaluation_details: Optional[Dict[str, Any]] = Field(None, alias="Evaluation Details")
    
    model_config = {
        "populate_by_name": True
    }
